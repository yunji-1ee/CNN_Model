{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":20270,"databundleVersionId":1222630,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Set input and output paths\ninput_data_path = \"/kaggle/input/siim-isic-melanoma-classification/jpeg/train\"\nlabels_csv_path = \"/kaggle/input/siim-isic-melanoma-classification/train.csv\"  # Ground truth 파일 경로","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.061530Z","iopub.execute_input":"2025-02-14T06:17:42.061943Z","iopub.status.idle":"2025-02-14T06:17:42.067521Z","shell.execute_reply.started":"2025-02-14T06:17:42.061908Z","shell.execute_reply":"2025-02-14T06:17:42.066196Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"# 라벨 데이터 로드 및 매핑 설정\ndf = pd.read_csv(labels_csv_path)\nlabel_map = {1: \"Melanoma\", 0: \"Benign\"}\ndf['label'] = df['target'].map(label_map)  # 'target' 컬럼에 따라 라벨 매핑","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.069316Z","iopub.execute_input":"2025-02-14T06:17:42.069825Z","iopub.status.idle":"2025-02-14T06:17:42.142822Z","shell.execute_reply.started":"2025-02-14T06:17:42.069782Z","shell.execute_reply":"2025-02-14T06:17:42.141692Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.144858Z","iopub.execute_input":"2025-02-14T06:17:42.145178Z","iopub.status.idle":"2025-02-14T06:17:42.159445Z","shell.execute_reply.started":"2025-02-14T06:17:42.145150Z","shell.execute_reply":"2025-02-14T06:17:42.158348Z"}},"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n\n  diagnosis benign_malignant  target   label  \n0   unknown           benign       0  Benign  \n1   unknown           benign       0  Benign  \n2     nevus           benign       0  Benign  \n3   unknown           benign       0  Benign  \n4   unknown           benign       0  Benign  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>patient_id</th>\n      <th>sex</th>\n      <th>age_approx</th>\n      <th>anatom_site_general_challenge</th>\n      <th>diagnosis</th>\n      <th>benign_malignant</th>\n      <th>target</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_2637011</td>\n      <td>IP_7279968</td>\n      <td>male</td>\n      <td>45.0</td>\n      <td>head/neck</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0015719</td>\n      <td>IP_3075186</td>\n      <td>female</td>\n      <td>45.0</td>\n      <td>upper extremity</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0052212</td>\n      <td>IP_2842074</td>\n      <td>female</td>\n      <td>50.0</td>\n      <td>lower extremity</td>\n      <td>nevus</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0068279</td>\n      <td>IP_6890425</td>\n      <td>female</td>\n      <td>45.0</td>\n      <td>head/neck</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0074268</td>\n      <td>IP_8723313</td>\n      <td>female</td>\n      <td>55.0</td>\n      <td>upper extremity</td>\n      <td>unknown</td>\n      <td>benign</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":104},{"cell_type":"code","source":"# 이미지 파일 리스트 생성 및 셔플\nfile_names = df['image_name'].tolist()\nfile_names = [os.path.join(input_data_path, f\"{name}.jpg\") for name in file_names]\nrandom.shuffle(file_names)\n\n# 이미지 파일에 해당하는 라벨 생성\nlabels = df['label'].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.161207Z","iopub.execute_input":"2025-02-14T06:17:42.161528Z","iopub.status.idle":"2025-02-14T06:17:42.245001Z","shell.execute_reply.started":"2025-02-14T06:17:42.161502Z","shell.execute_reply":"2025-02-14T06:17:42.244098Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"# 데이터셋 나누기\ntest_split_ratio = 0.2\nval_split_ratio = 0.3\n\n# 인덱스 계산\ntest_split_index = int(len(file_names) * (1 - test_split_ratio))\nval_split_index = int(test_split_index * (1 - val_split_ratio))\n\n# 파일 및 라벨 나누기\ntrain_files = file_names[:val_split_index]\ntrain_labels = labels[:val_split_index]\n\nval_files = file_names[val_split_index:test_split_index]\nval_labels = labels[val_split_index:test_split_index]\n\ntest_files = file_names[test_split_index:]\ntest_labels = labels[test_split_index:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.245869Z","iopub.execute_input":"2025-02-14T06:17:42.246189Z","iopub.status.idle":"2025-02-14T06:17:42.253111Z","shell.execute_reply.started":"2025-02-14T06:17:42.246163Z","shell.execute_reply":"2025-02-14T06:17:42.251921Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"import torch  # PyTorch library\nimport torch.nn as nn  # Module for neural networks\nimport torch.optim as optim  # Optimization algorithms\nimport torchvision.transforms as transforms  # Module for image transformations\nfrom torchvision import datasets  # Image datasets\nfrom torch.utils.data import DataLoader  # Data loaders \nimport matplotlib.pyplot as plt  # Visualization tool\nimport numpy as np  # Mathematical and array manipulation tool\nfrom collections import Counter  # Tool for counting elements\nimport torchvision.models as models  # Pre-trained models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score  # Evaluation metrics\nfrom sklearn.model_selection import train_test_split  # Dataset splitting\nimport optuna  # Hyperparameter optimization tool\nimport os  # Operating system-related tool\nimport csv  # Tool for handling CSV files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.254305Z","iopub.execute_input":"2025-02-14T06:17:42.254642Z","iopub.status.idle":"2025-02-14T06:17:42.274492Z","shell.execute_reply.started":"2025-02-14T06:17:42.254604Z","shell.execute_reply":"2025-02-14T06:17:42.273251Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"# Plotting the training and validation loss\ndef draw_train_val_curve(train_losses, val_losses, val_accuracies, val_micro_aurocs):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.title(\"Training and Validation Losses per Epoch\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig('train_val_losses_p_epoch.png')\n    plt.close()\n    plt.clf()\n\n    plt.figure(figsize=(10, 6))\n    val_accuracies_cpu = [acc.cpu().numpy() for acc in val_accuracies]\n    plt.plot(val_accuracies_cpu, label='Validation Accuracy')\n    plt.title('Validation Accuracy per Epoch')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.savefig('val_acc_p_epoch.png')\n    plt.close()\n    plt.clf()\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(val_micro_aurocs, label='Micro-average AUROC (Training)')\n    plt.title('Micro-average AUROC per Epoch (Training)')\n    plt.xlabel('Epochs')\n    plt.ylabel('AUROC')\n    plt.legend()\n    plt.savefig('val_micro_auroc_p_epoch.png')\n    plt.close()\n    plt.clf()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.275649Z","iopub.execute_input":"2025-02-14T06:17:42.275951Z","iopub.status.idle":"2025-02-14T06:17:42.293478Z","shell.execute_reply.started":"2025-02-14T06:17:42.275925Z","shell.execute_reply":"2025-02-14T06:17:42.292334Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"torch.set_num_threads(1)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nImage_Size = 227 \nNum_Epochs = 5\nLearning_Rate = 0.01 \nBatch_Size = 128\nModel_Name = 'Model_name'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.294681Z","iopub.execute_input":"2025-02-14T06:17:42.295025Z","iopub.status.idle":"2025-02-14T06:17:42.316669Z","shell.execute_reply.started":"2025-02-14T06:17:42.294998Z","shell.execute_reply":"2025-02-14T06:17:42.315656Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((227, 227)),          \n    transforms.ToTensor(), \n])\n\nval_transform = transforms.Compose([\n        transforms.Resize((227, 227)), \n        transforms.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.318767Z","iopub.execute_input":"2025-02-14T06:17:42.319040Z","iopub.status.idle":"2025-02-14T06:17:42.335387Z","shell.execute_reply.started":"2025-02-14T06:17:42.319017Z","shell.execute_reply":"2025-02-14T06:17:42.334275Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\n\n# 커스텀 데이터셋 클래스\nclass CustomImageDataset(Dataset):\n    def __init__(self, file_list, labels, transform=None):\n        self.file_list = file_list\n        self.labels = labels\n        self.transform = transform\n        self.classes = sorted(list(set(labels)))  # 클래스 이름 추출 및 정렬\n        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        img_path = self.file_list[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        label_idx = self.class_to_idx[label]\n        return image, label_idx\n\n# 데이터셋 및 데이터로더 생성\ntrain_dataset = CustomImageDataset(train_files, train_labels, transform=train_transform)\nval_dataset = CustomImageDataset(val_files, val_labels, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# 클래스 이름 및 개수 확인\nclass_names = train_dataset.classes\nnum_classes = len(class_names)\nprint(\"Class names:\", class_names)\nprint(\"Number of classes:\", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.336788Z","iopub.execute_input":"2025-02-14T06:17:42.337218Z","iopub.status.idle":"2025-02-14T06:17:42.355499Z","shell.execute_reply.started":"2025-02-14T06:17:42.337177Z","shell.execute_reply":"2025-02-14T06:17:42.354356Z"}},"outputs":[{"name":"stdout","text":"Class names: ['Benign', 'Melanoma']\nNumber of classes: 2\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"weights = models.ResNet50_Weights.IMAGENET1K_V1\nmodel = models.resnet50(weights=weights)\nclassifier = model.fc\nlast_layer_in_features = classifier.in_features\nmodel.fc = nn.Linear(last_layer_in_features, num_classes)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = Learning_Rate)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.356730Z","iopub.execute_input":"2025-02-14T06:17:42.357096Z","iopub.status.idle":"2025-02-14T06:17:42.950942Z","shell.execute_reply.started":"2025-02-14T06:17:42.357051Z","shell.execute_reply":"2025-02-14T06:17:42.949778Z"}},"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nbest_val_loss = float('inf')\nbest_val_acc = float(0.0)\nbest_val_auroc = float(0.0)\np_acc_counter = 0\np_loss_counter = 0\np_auroc_counter = 0\n\npatience = 300\ntrain_losses = []\nval_losses = []\nval_accuracies = []\nval_micro_aurocs = []\nlog_file_path = 'training_log.txt'\ncsv_file_path = 'training_metrics.csv'\n\nfor epoch in range(Num_Epochs):\n    model.train()\n    train_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * images.size(0)\n        \n    model.eval()\n    val_loss = 0.0\n    val_corrects = 0\n    val_labels = []\n    val_probas = []\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n\n            val_corrects += torch.sum(preds == labels.data)\n            val_labels.extend(labels.cpu().numpy())\n            val_probas.extend(probabilities.cpu().numpy())\n\n        val_labels = np.array(val_labels)\n        val_probas = np.array(val_probas)\n\n        # If labels are one-hot encoded, convert to class indices\n        if val_labels.ndim > 1:\n            val_labels = np.argmax(val_labels, axis=1)\n\n        train_loss = train_loss / len(train_loader.dataset)\n        val_loss = val_loss / len(val_loader.dataset)\n        val_accuracy = val_corrects.double() / len(val_loader.dataset)\n\n        # AUROC 계산\n        val_auroc = roc_auc_score(val_labels, val_probas[:, 1], average='micro')\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n        val_micro_aurocs.append(val_auroc)\n        draw_train_val_curve(train_losses, val_losses, val_accuracies, val_micro_aurocs)\n\n    # Write metrics to CSV file\n    with open(csv_file_path, 'a', newline='') as csv_file:\n        csv_writer = csv.writer(csv_file)\n        csv_writer.writerow([epoch + 1, train_loss, val_auroc, val_loss, val_accuracy])\n\n    # Check for improvement\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'valloss.pth')\n        p_loss_counter = 0\n    else:\n        p_loss_counter += 1\n\n    if val_accuracy > best_val_acc:\n        best_val_acc = val_accuracy\n        torch.save(model.state_dict(), 'valacc.pth')\n        p_acc_counter = 0\n    else:\n        p_acc_counter += 1\n    \n    if val_auroc > best_val_auroc:\n        best_val_auroc = val_auroc\n        torch.save(model.state_dict(), 'auroc.pth')\n        p_auroc_counter = 0\n    else:\n        p_auroc_counter += 1\n\n    # Early stopping check (수정)\n    if p_loss_counter >= patience and p_acc_counter >= patience and p_auroc_counter >= patience:\n        print(\"Stopping early due to no improvement in validation metrics.\")\n        break\n\n    torch.save(model.state_dict(), 'epoch.pth')\n\n    # Log training progress\n    with open(log_file_path, 'a') as log_file:\n        log_file.write(f\"Epoch {epoch+1}/{Num_Epochs}, Train Loss: {train_loss:.4f}, Val auroc: {val_auroc:.4f}, \"\n                       f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, \"\n                       f\"patience counter (acc, loss, auroc): {p_acc_counter}, {p_loss_counter}, {p_auroc_counter}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T06:17:42.952215Z","iopub.execute_input":"2025-02-14T06:17:42.952652Z"}},"outputs":[],"execution_count":null}]}