{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import for testcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights, ResNet50_Weights, resnet50\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainData Preprocess with dataLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name = 'RESNET'\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((227, 227)), \n",
    "        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "Batch_Size = 4\n",
    "\n",
    "test_dir = \"your test dataset\"\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Batch_Size, shuffle =False)\n",
    "class_names = test_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "model = models.resnet50(weights=weights)\n",
    "classifier = model.fc\n",
    "last_layer_in_features = classifier.in_features\n",
    "model.fc = nn.Linear(last_layer_in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load\n",
    "youn need train models like **.pth  \n",
    "here we use auroc.pth for test  \n",
    "you can use anything which you want to test   \n",
    "ex) loss, accuracy.. by val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../auroc.pth'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating \n",
    "\n",
    "aim to calculate model\n",
    "such as precision, recall, f1 etc..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_metrics(all_labels, all_preds, average):\n",
    "    overall_precision = precision_score(\n",
    "        all_labels, all_preds, average=average, zero_division=0)\n",
    "    overall_recall = recall_score(\n",
    "        all_labels, all_preds, average=average, zero_division=0)\n",
    "    overall_f1 = f1_score(all_labels, all_preds,\n",
    "                          average=average, zero_division=0)\n",
    "    return overall_precision, overall_recall, overall_f1\n",
    "\n",
    "def calculate_class_metrics(class_labels, class_preds):\n",
    "    precision = precision_score(class_labels, class_preds, zero_division=0)\n",
    "    recall = recall_score(class_labels, class_preds, zero_division=0)\n",
    "    f1 = f1_score(class_labels, class_preds, zero_division=0)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    absolute_errors = np.abs(y_true - y_pred)\n",
    "    mae = np.mean(absolute_errors)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Initialize lists for result storage\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "f1_scores = []\n",
    "all_proba = []\n",
    "\n",
    "# Initialize labels and predictions for each class\n",
    "class_labels = [[], [], [], []]\n",
    "class_preds = [[], [], [], []]\n",
    "class_probas = [[], [], [], []]\n",
    "class_probas_idx = [[], [], [], []]\n",
    "\n",
    "# Disable automatic gradient computation\n",
    "with torch.no_grad():\n",
    "    # Create CSV file and write header\n",
    "    data_record_csv = \"test_data_record.csv\"\n",
    "    with open(data_record_csv, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"True Label\", \"Data name\", \"MildDemented\", \"ModerateDemented\", \"NonDemented\", \"VeryMildDemented\"])\n",
    "        \n",
    "        # Batch processing through data loader\n",
    "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to appropriate device\n",
    "            outputs = model(images)  # Perform prediction through the model\n",
    "            _, preds = torch.max(outputs, 1)  # Extract class with highest probability\n",
    "            probabilities = F.softmax(outputs, dim=1)  # Calculate probabilities for each class\n",
    "\n",
    "            # Store results\n",
    "            all_preds.extend(preds.cpu().numpy())  # Store prediction results\n",
    "            all_labels.extend(labels.cpu().numpy())  # Store actual labels\n",
    "            all_proba.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "            \n",
    "            # For each image in the batch\n",
    "            for i, (label, pred, prob) in enumerate(zip(labels, preds, probabilities)):\n",
    "                class_labels[label].append(label.cpu().item())\n",
    "                class_preds[label].append(pred.cpu().item())\n",
    "                class_probas[label].append(prob.cpu().numpy())\n",
    "                # Get file path of current image\n",
    "                \n",
    "                img_path = test_dataset.samples[batch_idx * Batch_Size + i][0]\n",
    "                # Extract only file name\n",
    "                img_name = os.path.basename(img_path)\n",
    "                \n",
    "                # Write to CSV file\n",
    "                formatted_probs = [\"{:.2f}\".format(p.item() * 100) for p in prob]\n",
    "                csv_writer.writerow([label.item(), img_name, *formatted_probs])\n",
    "\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_proba = np.array(all_proba)\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions and labels saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n",
    "\n",
    "# plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={\"size\": 20})\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_mat.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for each class\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "class_labels_list = []\n",
    "class_probas_list = []\n",
    "\n",
    "table_rows = []\n",
    "total_class_acc = 0\n",
    "class_top2_recall_list = []\n",
    "top2_recall_total = 0\n",
    "balance_acc_total = 0\n",
    "total_mae = 0\n",
    "\n",
    "top_k_acc_csv = model_name+'_top_k_pred.csv'\n",
    "total_macro_auroc = 0\n",
    "\n",
    "\n",
    "with open(top_k_acc_csv, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Calculate metrics for each class\n",
    "\n",
    "    for class_index in range(4):  # Replace num_classes with the actual number of classes\n",
    "        print('class index : ', class_index)\n",
    "        class_preds_bin = (all_preds == class_index)\n",
    "        class_labels_bin = (all_labels == class_index)\n",
    "        class_probas_bin = all_proba[:, class_index]\n",
    "        for class_label in class_labels_bin:\n",
    "            class_labels_list.append(class_label)\n",
    "        for class_proba in class_probas_bin:\n",
    "            class_probas_list.append(class_proba)\n",
    "            \n",
    "        second_max_list = []\n",
    "        for array in class_probas[class_index]:\n",
    "            second_max = np.partition(array,-2)[-2]\n",
    "            second_max_idx = np.where(array == second_max)[0][0]\n",
    "            second_max_list.append(second_max_idx)\n",
    "        \n",
    "        mae = mean_absolute_error(class_labels[class_index],class_preds[class_index])\n",
    "        total_mae += mae\n",
    "        \n",
    "        csv_writer.writerow([\"class_labels\", class_labels[class_index]])\n",
    "        csv_writer.writerow([\"class_preds\", class_preds[class_index]])\n",
    "        csv_writer.writerow([\"second_max_list\", second_max_list])\n",
    "        csv_writer.writerow([\"mae\",mae])\n",
    "        csv_writer.writerow([])\n",
    "        \n",
    "        fpr[class_index], tpr[class_index], _ = roc_curve(\n",
    "            class_labels_bin, class_probas_bin)\n",
    "        # Calculate the area under the ROC curve (AUC)\n",
    "        roc_auc[class_index] = auc(fpr[class_index], tpr[class_index])\n",
    "        total_macro_auroc += roc_auc[class_index]\n",
    "\n",
    "        accuracy = cm[class_index][class_index] / np.sum(cm[class_index])\n",
    "        balance_acc_total += accuracy\n",
    "        \n",
    "        precision, recall, f1 = calculate_class_metrics(\n",
    "        class_labels_bin, class_preds_bin)\n",
    "        f1_scores.append(f1)\n",
    "        class_name = class_names[class_index]\n",
    "\n",
    "        count_top2_idx = np.sum(np.array(second_max_list) == class_index)\n",
    "        top2_recall = (cm[class_index][class_index] + count_top2_idx) / np.sum(cm[class_index])\n",
    "        top2_recall_total += top2_recall\n",
    "\n",
    "        class_top2_recall_list.append(top2_recall)\n",
    "        table_rows.append([class_name, roc_auc[class_index], mae, precision, recall, f1, top2_recall])\n",
    "\n",
    "avg_mae = total_mae / 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(class_labels_list, class_probas_list)\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "roc_auc[\"macro\"] = total_macro_auroc / 4\n",
    "\n",
    "\n",
    "all_fpr = np.unique(np.concatenate(\n",
    "    [fpr[class_index] for class_index in range(num_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for class_index in range(num_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[class_index], tpr[class_index])\n",
    "    \n",
    "    \n",
    "mean_tpr = mean_tpr/num_classes\n",
    "\n",
    "macro_top2_recall = top2_recall_total / 4\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    plt.plot(fpr[class_index], tpr[class_index],\n",
    "             label='ROC curve[{0}]:{1} (area = {2:0.4f})'\n",
    "             ''.format(class_index, class_names[class_index], roc_auc[class_index]))\n",
    "\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = calculate_avg_metrics(all_labels, all_preds, \"micro\")\n",
    "macro_precision, macro_recall, macro_f1 = calculate_avg_metrics(all_labels, all_preds, \"macro\")\n",
    "\n",
    "\n",
    "micro_top2_recall = top_k_accuracy_score(all_labels,all_proba,k=2)\n",
    "# Calculate micro and macro accuracy\n",
    "micro_accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_accuracy = np.mean([accuracy_score(all_labels[all_labels == i], all_preds[all_labels == i]) for i in range(num_classes)])\n",
    "\n",
    "# Add micro and macro accuracy to the metrics table\n",
    "micro_metrics = [roc_auc['micro'], \"\", micro_precision, micro_recall, micro_f1, micro_top2_recall, micro_accuracy]\n",
    "macro_metrics = [roc_auc[\"macro\"], avg_mae, macro_precision, macro_recall, macro_f1, macro_top2_recall, macro_accuracy]\n",
    "\n",
    "# Append micro and macro metrics to table rows\n",
    "table_rows.append([\"Macro\", *macro_metrics])\n",
    "table_rows.append([\"Micro\", *micro_metrics])\n",
    "\n",
    "# Define table headers to include Accuracy\n",
    "table_headers = [\"Class\", \"Auroc\", \"MAE\", \"Precision\", \"Recall\", \"F1 Score\", \"Top2 Recall\", \"Accuracy\"]\n",
    "print(tabulate(table_rows, headers=table_headers, tablefmt=\"pretty\"))\n",
    "\n",
    "# Plotting and saving the figure as before\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Each Class')\n",
    "plt.legend()\n",
    "plt.savefig(model_name+'_auroc.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "csv_file_path = model_name+'_metrics_results.csv'\n",
    "csv_class_names = class_names\n",
    "\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Write header\n",
    "    csv_writer.writerow([\"Class\", \"Auroc\", \"MAE\", \"Precision\", \"Recall\", \"F1 Score\" ,\"Top2 Recall\" ,\"Accuracy\"])\n",
    "    # Write class-wise metrics\n",
    "    for class_index in range(4):\n",
    "        csv_writer.writerow([\n",
    "            class_names[class_index],\n",
    "            \"{:.3f}\".format(table_rows[class_index][1]),  # auroc\n",
    "            \"{:.3f}\".format(table_rows[class_index][2]),  # mae\n",
    "            \"{:.3f}\".format(table_rows[class_index][3]),  # precision\n",
    "            \"{:.3f}\".format(table_rows[class_index][4]),  # recall\n",
    "            \"{:.3f}\".format(table_rows[class_index][5]),  # f1\n",
    "            \"{:.3f}\".format(table_rows[class_index][6]),  # top2 recall\n",
    "        ])\n",
    "        \n",
    "    csv_writer.writerow([\n",
    "        'macro',\n",
    "        \"{:.3f}\".format(roc_auc[\"macro\"]),  # auroc\n",
    "        \"{:.3f}\".format(macro_metrics[1]),  # mae\n",
    "        \"{:.3f}\".format(macro_metrics[2]),  # precision\n",
    "        \"{:.3f}\".format(macro_metrics[3]),  # recall\n",
    "        \"{:.3f}\".format(macro_metrics[4]),  # f1\n",
    "        \"{:.3f}\".format(macro_metrics[5]),  # top2 recall\n",
    "        \"{:.3f}\".format(macro_metrics[6]),  # accuracy\n",
    "    ])\n",
    "    csv_writer.writerow([\n",
    "        'micro',\n",
    "        \"{:.3f}\".format(micro_metrics[0]),  # auroc\n",
    "        (micro_metrics[1]),  # mae\n",
    "        \"{:.3f}\".format(micro_metrics[2]),  # precision\n",
    "        \"{:.3f}\".format(micro_metrics[3]),  # recall\n",
    "        \"{:.3f}\".format(micro_metrics[4]),  # f1\n",
    "        \"{:.3f}\".format(micro_metrics[5]),  # top2 recall\n",
    "        \"{:.3f}\".format(micro_metrics[6]),  # accuracy\n",
    "    ])\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "csv_full_file_path = model_name+'_full_metrics_results.csv'\n",
    "csv_full_class_names = class_names\n",
    "\n",
    "with open(csv_full_file_path, 'w', newline='') as csv_file_full:\n",
    "    csv_writer = csv.writer(csv_file_full)\n",
    "    # Write header\n",
    "    csv_writer.writerow([\"Class\", \"Auroc\", \"MAE\", \"Precision\", \"Recall\", \"F1 Score\" ,\"Top2 Recall\",\"Accuracy\"])\n",
    "    # Write class-wise metrics\n",
    "    for class_index in range(4):\n",
    "        csv_writer.writerow([\n",
    "            class_names[class_index],\n",
    "            (table_rows[class_index][1]),  # auroc\n",
    "            (table_rows[class_index][2]),  # mae\n",
    "            (table_rows[class_index][3]),  # precision\n",
    "            (table_rows[class_index][4]),  # recall\n",
    "            (table_rows[class_index][5]),  # f1\n",
    "            (table_rows[class_index][6]),  # top2 recall\n",
    "        ])\n",
    "    csv_writer.writerow([\n",
    "        'macro',\n",
    "        (total_macro_auroc/7),  # auroc\n",
    "        (macro_metrics[1]),  # mae\n",
    "        (macro_metrics[2]),  # precision\n",
    "        (macro_metrics[3]),  # recall\n",
    "        (macro_metrics[4]),  # f1\n",
    "        (macro_metrics[5]),  # top2 recall\n",
    "        (macro_metrics[6]),  # accuracy\n",
    "        \n",
    "    ])\n",
    "    csv_writer.writerow([\n",
    "        'micro',\n",
    "        (micro_metrics[0]),  # auroc\n",
    "        (micro_metrics[1]),  # mae\n",
    "        (micro_metrics[2]),  # precision\n",
    "        (micro_metrics[3]),  # recall\n",
    "        (micro_metrics[4]),  # f1\n",
    "        (micro_metrics[5]),  # top2 recall\n",
    "        (micro_metrics[6]),  # accuracy\n",
    "    ])\n",
    "\n",
    "csv_file_full.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
